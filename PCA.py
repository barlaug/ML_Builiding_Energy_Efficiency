from cProfile import label
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.linalg import svd
from sklearn.preprocessing import StandardScaler
import seaborn as sns


# Load the csv data using the Pandas library
filename = 'ENB2012_data.csv'
df = pd.read_csv(filename)


#Convert the data frame to a numpy array 
raw_data = df.values

#X is all the attributes, except the heating and cooling load (last two columns)
#y is the class labels, i.e. the cooling load

X = raw_data[:,:8]

#Extract the attribute names
attributeNames = np.asarray(df.columns[:8])

#The class Labels are  the last column (the cooling load)
classLabels = raw_data[:,-1]

#Discretize the cooling load, which is used for classification
nbClassCL = 4
discrete_CL = []
for cl in classLabels:
    if cl <= 20:
        discrete_CL.append('1')
    elif cl <= 30:
        discrete_CL.append('2')
    elif cl <= 40:
        discrete_CL.append('3')
    else:
        discrete_CL.append('4')


#unique class labels
classNames = np.unique(discrete_CL)

#making a dictionary in order to assign each classlabel a unique number
classDict = {"1": 3, "2": 2, "3": 1, "4": 0}

#class index vector y 
y = np.array([classDict[cl] for cl in discrete_CL])

#N = data objects, M = attributes, C = number of classes
N, M = X.shape
C = len(classNames)

#PCA is effected by scale so we need to scale the features in the data before applying PCA. 
#Standardize the dataset’s features onto unit scale (mean = 0 and variance = 1) 
Y = StandardScaler().fit_transform(X)

# PCA by computing SVD of Y
#svd factorizes the matrix X into two unitary matrices U and Vh, 
# and a 1D array s of singular values (real, non-negative) such that X == U @ S @ Vh, 
# where S is a suitably shaped matrix of zeros with main diagonal s.
U,S,Vh = svd(Y,full_matrices=False)

#The columns of V gives  the principal component directions, i.e. in which direction the data varies
V = Vh.T

# Compute variance explained by principal components
rho = (S*S) / (S*S).sum() 

threshold = 0.9

# Plot how much of the total variance is “explained” by each principal component generated by the PCA
plt.figure()
plt.plot(range(1,len(rho)+1),rho,'x-')
plt.plot(range(1,len(rho)+1),np.cumsum(rho),'o-')
plt.plot([1,len(rho)],[threshold, threshold],'k--')
plt.title('Variance explained by principal components');
plt.xlabel('Principal component');
plt.ylabel('Variance explained');
plt.legend(['Individual','Cumulative','Threshold'])
plt.grid()


#Plot the projection
pc_proj = V.dot(Y.T)
df_cl = pd.DataFrame(pc_proj.T).iloc[:, :4]
df_cl.columns = ['PC1', 'PC2', 'PC3', 'PC4']
df_cl['Cooling load'] = discrete_CL
g = sns.pairplot(df_cl, hue='Cooling load')
#Changing the colors 
colors = ["#2ca02c", "#1f77b4", "#ff7f0e", "#d62728"]
sns.set_palette(sns.color_palette(colors))


# Plot the PCA component coefficients
plt.figure()
pcs = [0,1,2,3] 
legendStrs = ['PC'+str(e+1) for e in pcs]
bw = .2
r = np.arange(1,M+1)
for i in pcs:    
    plt.bar(r+i*bw, V[:,i], width=bw)
plt.xticks(r+bw, attributeNames)
plt.xlabel('Attributes')
plt.ylabel('Component coefficients')
plt.legend(legendStrs)
plt.grid()
plt.title('PCA Component Coefficients')
plt.show()

