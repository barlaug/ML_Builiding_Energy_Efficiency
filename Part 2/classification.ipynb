{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this script, the heating load is classified and the result is evaluated statistically.\n",
    "We compare a baseline, a logistic regression model and an KNN model. The following solves a multiclass classification problem where a discretized version of the heating load is our target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read/prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relative_compactness</th>\n",
       "      <th>surface_area</th>\n",
       "      <th>wall_area</th>\n",
       "      <th>roof_area</th>\n",
       "      <th>overall_height</th>\n",
       "      <th>orientation</th>\n",
       "      <th>glazing_area</th>\n",
       "      <th>glazing_area_distribution</th>\n",
       "      <th>heating_load</th>\n",
       "      <th>cooling_load</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.98</td>\n",
       "      <td>514.5</td>\n",
       "      <td>294.0</td>\n",
       "      <td>110.25</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>15.55</td>\n",
       "      <td>21.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.98</td>\n",
       "      <td>514.5</td>\n",
       "      <td>294.0</td>\n",
       "      <td>110.25</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>15.55</td>\n",
       "      <td>21.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.98</td>\n",
       "      <td>514.5</td>\n",
       "      <td>294.0</td>\n",
       "      <td>110.25</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>15.55</td>\n",
       "      <td>21.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.98</td>\n",
       "      <td>514.5</td>\n",
       "      <td>294.0</td>\n",
       "      <td>110.25</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>15.55</td>\n",
       "      <td>21.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.90</td>\n",
       "      <td>563.5</td>\n",
       "      <td>318.5</td>\n",
       "      <td>122.50</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>20.84</td>\n",
       "      <td>28.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>0.64</td>\n",
       "      <td>784.0</td>\n",
       "      <td>343.0</td>\n",
       "      <td>220.50</td>\n",
       "      <td>3.5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>5</td>\n",
       "      <td>17.88</td>\n",
       "      <td>21.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>0.62</td>\n",
       "      <td>808.5</td>\n",
       "      <td>367.5</td>\n",
       "      <td>220.50</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>5</td>\n",
       "      <td>16.54</td>\n",
       "      <td>16.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>0.62</td>\n",
       "      <td>808.5</td>\n",
       "      <td>367.5</td>\n",
       "      <td>220.50</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>5</td>\n",
       "      <td>16.44</td>\n",
       "      <td>17.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>0.62</td>\n",
       "      <td>808.5</td>\n",
       "      <td>367.5</td>\n",
       "      <td>220.50</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>5</td>\n",
       "      <td>16.48</td>\n",
       "      <td>16.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>0.62</td>\n",
       "      <td>808.5</td>\n",
       "      <td>367.5</td>\n",
       "      <td>220.50</td>\n",
       "      <td>3.5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>5</td>\n",
       "      <td>16.64</td>\n",
       "      <td>16.03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>768 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     relative_compactness  surface_area  wall_area  roof_area  overall_height  \\\n",
       "0                    0.98         514.5      294.0     110.25             7.0   \n",
       "1                    0.98         514.5      294.0     110.25             7.0   \n",
       "2                    0.98         514.5      294.0     110.25             7.0   \n",
       "3                    0.98         514.5      294.0     110.25             7.0   \n",
       "4                    0.90         563.5      318.5     122.50             7.0   \n",
       "..                    ...           ...        ...        ...             ...   \n",
       "763                  0.64         784.0      343.0     220.50             3.5   \n",
       "764                  0.62         808.5      367.5     220.50             3.5   \n",
       "765                  0.62         808.5      367.5     220.50             3.5   \n",
       "766                  0.62         808.5      367.5     220.50             3.5   \n",
       "767                  0.62         808.5      367.5     220.50             3.5   \n",
       "\n",
       "     orientation  glazing_area  glazing_area_distribution  heating_load  \\\n",
       "0              2           0.0                          0         15.55   \n",
       "1              3           0.0                          0         15.55   \n",
       "2              4           0.0                          0         15.55   \n",
       "3              5           0.0                          0         15.55   \n",
       "4              2           0.0                          0         20.84   \n",
       "..           ...           ...                        ...           ...   \n",
       "763            5           0.4                          5         17.88   \n",
       "764            2           0.4                          5         16.54   \n",
       "765            3           0.4                          5         16.44   \n",
       "766            4           0.4                          5         16.48   \n",
       "767            5           0.4                          5         16.64   \n",
       "\n",
       "     cooling_load  \n",
       "0           21.33  \n",
       "1           21.33  \n",
       "2           21.33  \n",
       "3           21.33  \n",
       "4           28.28  \n",
       "..            ...  \n",
       "763         21.40  \n",
       "764         16.88  \n",
       "765         17.11  \n",
       "766         16.61  \n",
       "767         16.03  \n",
       "\n",
       "[768 rows x 10 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relative_compactness</th>\n",
       "      <th>surface_area</th>\n",
       "      <th>wall_area</th>\n",
       "      <th>roof_area</th>\n",
       "      <th>overall_height</th>\n",
       "      <th>orientation</th>\n",
       "      <th>glazing_area</th>\n",
       "      <th>glazing_area_distribution</th>\n",
       "      <th>heating_load</th>\n",
       "      <th>cooling_load</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.00000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.00000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.764167</td>\n",
       "      <td>671.708333</td>\n",
       "      <td>318.500000</td>\n",
       "      <td>176.604167</td>\n",
       "      <td>5.25000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>0.234375</td>\n",
       "      <td>2.81250</td>\n",
       "      <td>22.307201</td>\n",
       "      <td>24.587760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.105777</td>\n",
       "      <td>88.086116</td>\n",
       "      <td>43.626481</td>\n",
       "      <td>45.165950</td>\n",
       "      <td>1.75114</td>\n",
       "      <td>1.118763</td>\n",
       "      <td>0.133221</td>\n",
       "      <td>1.55096</td>\n",
       "      <td>10.090196</td>\n",
       "      <td>9.513306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.620000</td>\n",
       "      <td>514.500000</td>\n",
       "      <td>245.000000</td>\n",
       "      <td>110.250000</td>\n",
       "      <td>3.50000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>6.010000</td>\n",
       "      <td>10.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.682500</td>\n",
       "      <td>606.375000</td>\n",
       "      <td>294.000000</td>\n",
       "      <td>140.875000</td>\n",
       "      <td>3.50000</td>\n",
       "      <td>2.750000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.75000</td>\n",
       "      <td>12.992500</td>\n",
       "      <td>15.620000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>673.750000</td>\n",
       "      <td>318.500000</td>\n",
       "      <td>183.750000</td>\n",
       "      <td>5.25000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>18.950000</td>\n",
       "      <td>22.080000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.830000</td>\n",
       "      <td>741.125000</td>\n",
       "      <td>343.000000</td>\n",
       "      <td>220.500000</td>\n",
       "      <td>7.00000</td>\n",
       "      <td>4.250000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>31.667500</td>\n",
       "      <td>33.132500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.980000</td>\n",
       "      <td>808.500000</td>\n",
       "      <td>416.500000</td>\n",
       "      <td>220.500000</td>\n",
       "      <td>7.00000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>5.00000</td>\n",
       "      <td>43.100000</td>\n",
       "      <td>48.030000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       relative_compactness  surface_area   wall_area   roof_area  \\\n",
       "count            768.000000    768.000000  768.000000  768.000000   \n",
       "mean               0.764167    671.708333  318.500000  176.604167   \n",
       "std                0.105777     88.086116   43.626481   45.165950   \n",
       "min                0.620000    514.500000  245.000000  110.250000   \n",
       "25%                0.682500    606.375000  294.000000  140.875000   \n",
       "50%                0.750000    673.750000  318.500000  183.750000   \n",
       "75%                0.830000    741.125000  343.000000  220.500000   \n",
       "max                0.980000    808.500000  416.500000  220.500000   \n",
       "\n",
       "       overall_height  orientation  glazing_area  glazing_area_distribution  \\\n",
       "count       768.00000   768.000000    768.000000                  768.00000   \n",
       "mean          5.25000     3.500000      0.234375                    2.81250   \n",
       "std           1.75114     1.118763      0.133221                    1.55096   \n",
       "min           3.50000     2.000000      0.000000                    0.00000   \n",
       "25%           3.50000     2.750000      0.100000                    1.75000   \n",
       "50%           5.25000     3.500000      0.250000                    3.00000   \n",
       "75%           7.00000     4.250000      0.400000                    4.00000   \n",
       "max           7.00000     5.000000      0.400000                    5.00000   \n",
       "\n",
       "       heating_load  cooling_load  \n",
       "count    768.000000    768.000000  \n",
       "mean      22.307201     24.587760  \n",
       "std       10.090196      9.513306  \n",
       "min        6.010000     10.900000  \n",
       "25%       12.992500     15.620000  \n",
       "50%       18.950000     22.080000  \n",
       "75%       31.667500     33.132500  \n",
       "max       43.100000     48.030000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "datapath = \"data/\"\n",
    "filename = 'ENB2012_data.csv'\n",
    "df = pd.read_csv(datapath+filename)\n",
    "df.columns = ['relative_compactness', 'surface_area', 'wall_area', 'roof_area', 'overall_height', 'orientation', 'glazing_area', 'glazing_area_distribution', 'heating_load', 'cooling_load']\n",
    "display(df)\n",
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: >"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Discretize the Heating Load (HL), done manually by looking at the ranges of the data\n",
    "discrete_HL = []\n",
    "for i, row in df.iterrows():\n",
    "    hl = row['heating_load']\n",
    "    if hl <= 10:\n",
    "        discrete_HL.append(1)\n",
    "    elif hl <= 20:\n",
    "        discrete_HL.append(2)\n",
    "    elif hl <= 30:\n",
    "        discrete_HL.append(3)\n",
    "    else:\n",
    "        discrete_HL.append(4)\n",
    "        \n",
    "hl_df = df.iloc[: , :8]\n",
    "hl_df['discrete_heating_load'] = discrete_HL\n",
    "hl_df['discrete_heating_load'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set target and data\n",
    "y = hl_df['discrete_heating_load']\n",
    "X = hl_df.drop([\"discrete_heating_load\"], axis=1)\n",
    "\n",
    "# Standardize training data, pandas automatically applies functions column-wise\n",
    "X = (X - X.mean())/X.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare models: Two-level (nested) cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For baseline: Compute the largest class on the training data, and predict everything in the test data as belonging to that class. \n",
    "<br/>-> corresponding to logistic regression with bias term and no features.\n",
    "\n",
    "For multinomial regression: Inner fold is estimating lambda, the complexity controlling parameter (called C in sklearn)\n",
    "\n",
    "For KNN: Inner fold is estimating K, the number of neighbours in the algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold, GridSearchCV, cross_val_score, train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_level_cross_validation(X, y, k1, k2, p_grid_knn, p_grid_reg):\n",
    "    \"\"\"Performs a two level cross validation for logistic regression, KNN and a baseline classifier.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: dataframe\n",
    "        The input feature from which we want to make the prediction\n",
    "\n",
    "    y: single column dataframe\n",
    "        The target for X\n",
    "    \n",
    "    k1: int\n",
    "        The number of folds for the first level of the cross validation\n",
    "    \n",
    "    k2: int\n",
    "        The number of folds for the second level of the cross validation\n",
    "\n",
    "    p_grid_knn: dictionary{string, list of values}\n",
    "        The parameters to be tested and compared on the model for KNN\n",
    "    \n",
    "    p_grid_reg: dictionary{string, list of values}\n",
    "        The parameters to be tested and compared on the model for KNN\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    res: dataframe\n",
    "        Returns the best parameters found at each fold with the associated error for each model.\n",
    "\n",
    "    \"\"\"\n",
    "    # Define table for output\n",
    "    res = {\"outer_fold\": [], \"KNN_k_i\": [], \"KNN_test_error_i\" : [], \"reg_lambda_i\" : [], \"reg_test_error_i\" : [], \"baseline_test_error_i\": []}\n",
    "    res[\"outer_fold\"] = range(k1)\n",
    "\n",
    "    dummy_errors = []\n",
    "    cv_outer = KFold(n_splits=k1, shuffle=True) #outer cross validation obj\n",
    "    for train_ix, test_ix in cv_outer.split(X):\n",
    "        # Split data\n",
    "        x_train, x_test = X.iloc[train_ix,:], X.iloc[test_ix,:]\n",
    "        y_train, y_test = y.iloc[train_ix], y.iloc[test_ix]\n",
    "        \n",
    "        # Do baseline estimation error and append to table\n",
    "        dummy = DummyClassifier(strategy='most_frequent')\n",
    "        dummy.fit(x_train, y_train)\n",
    "        dummy_errors.append(1 - dummy.score(x_test, y_test))\n",
    "\n",
    "        \n",
    "        cv_inner = KFold(n_splits=k2, shuffle=True) #inner cross validation obj\n",
    "\n",
    "        # Model objects\n",
    "        knn = KNeighborsClassifier()\n",
    "        reg = LogisticRegression(multi_class='multinomial')\n",
    "\n",
    "        # Exhaustive search over specified parameter values (p_grid_...) (with the inner cross validation) for the estimators\n",
    "        search_knn = GridSearchCV(knn, p_grid_knn, scoring='accuracy', cv=cv_inner, refit=True)\n",
    "        search_reg = GridSearchCV(reg, p_grid_reg, scoring='accuracy', cv=cv_inner, refit=True)\n",
    "\n",
    "        # Do actual search on the current training split\n",
    "        res_knn = search_knn.fit(x_train, y_train)\n",
    "        res_reg = search_reg.fit(x_train, y_train)\n",
    "\n",
    "        # Get best performing model on training set from inner fold\n",
    "        best_knn = res_knn.best_estimator_\n",
    "        best_reg = res_reg.best_estimator_\n",
    "\n",
    "        # Get the parameter for the best performing model within the fold\n",
    "        best_param_knn = res_knn.best_params_['n_neighbors'] #i.e. K\n",
    "        best_param_reg = res_reg.best_params_['C'] #i.e. lambda\n",
    "\n",
    "        # Evaluate best scoring estimators on training set\n",
    "        y_pred_knn = best_knn.predict(x_test)\n",
    "        y_pred_reg = best_reg.predict(x_test)\n",
    "        \n",
    "        # Check error for best estimators on current test split\n",
    "        err_knn = 1 - accuracy_score(y_test, y_pred_knn)\n",
    "        err_reg = 1 - accuracy_score(y_test, y_pred_reg)\n",
    "        \n",
    "        # Append values to respective lists in outside dict\n",
    "        res[\"KNN_k_i\"].append(best_param_knn)\n",
    "        res[\"KNN_test_error_i\"].append(err_knn)\n",
    "        res[\"reg_lambda_i\"].append(best_param_reg)\n",
    "        res[\"reg_test_error_i\"].append(err_reg)\n",
    "    \n",
    "    # insert baseline errors for all i into dict\n",
    "    res['baseline_test_error_i'] = dummy_errors\n",
    "\n",
    "    # returned transformed dict as pandas df\n",
    "    return pd.DataFrame.from_dict(data=res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of folds in inner and outer cross validation\n",
    "k_1, k_2 = 10, 10\n",
    "\n",
    "# Parameter grids. I.e. what vals to check for optimality in respective model (for inner folds)\n",
    "# Parameter grids are based on trial and error\n",
    "p_grid_knn = {\"n_neighbors\": [1, 2, 3, 4, 5, 6, 7, 8, 16, 32]} \n",
    "p_grid_reg = {\"C\": range(10, 20, 1)}\n",
    "\n",
    "cross_val_table = two_level_cross_validation(X, y, k_1, k_2, p_grid_knn, p_grid_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>outer_fold</th>\n",
       "      <th>KNN_k_i</th>\n",
       "      <th>KNN_test_error_i</th>\n",
       "      <th>reg_lambda_i</th>\n",
       "      <th>reg_test_error_i</th>\n",
       "      <th>baseline_test_error_i</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.025974</td>\n",
       "      <td>10</td>\n",
       "      <td>0.051948</td>\n",
       "      <td>0.467532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.038961</td>\n",
       "      <td>10</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.493506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.025974</td>\n",
       "      <td>13</td>\n",
       "      <td>0.038961</td>\n",
       "      <td>0.532468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.025974</td>\n",
       "      <td>12</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.532468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.025974</td>\n",
       "      <td>16</td>\n",
       "      <td>0.064935</td>\n",
       "      <td>0.441558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.025974</td>\n",
       "      <td>13</td>\n",
       "      <td>0.103896</td>\n",
       "      <td>0.506494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.025974</td>\n",
       "      <td>13</td>\n",
       "      <td>0.103896</td>\n",
       "      <td>0.493506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.038961</td>\n",
       "      <td>11</td>\n",
       "      <td>0.103896</td>\n",
       "      <td>0.558442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>10</td>\n",
       "      <td>0.065789</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>10</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.565789</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   outer_fold  KNN_k_i  KNN_test_error_i  reg_lambda_i  reg_test_error_i  \\\n",
       "0           0        1          0.025974            10          0.051948   \n",
       "1           1        3          0.038961            10          0.090909   \n",
       "2           2        1          0.025974            13          0.038961   \n",
       "3           3        3          0.025974            12          0.090909   \n",
       "4           4        1          0.025974            16          0.064935   \n",
       "5           5        1          0.025974            13          0.103896   \n",
       "6           6        1          0.025974            13          0.103896   \n",
       "7           7        1          0.038961            11          0.103896   \n",
       "8           8        1          0.052632            10          0.065789   \n",
       "9           9        1          0.078947            10          0.052632   \n",
       "\n",
       "   baseline_test_error_i  \n",
       "0               0.467532  \n",
       "1               0.493506  \n",
       "2               0.532468  \n",
       "3               0.532468  \n",
       "4               0.441558  \n",
       "5               0.506494  \n",
       "6               0.493506  \n",
       "7               0.558442  \n",
       "8               0.500000  \n",
       "9               0.565789  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(cross_val_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical evaluation: McNemar's test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the three previously obtained models pairwise with a *McNemar's test*. The McNemar’s test is a paired nonparametric or distribution-free statistical hypothesis test. We find p-values and confidence intervals for the pairwise tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the two-level cross-validation table obtained above, and multiple runs of the code, we choose the hyperparameters for the kNN and LogisticRegression (Multinomial regression, that is) model that seems optimal. That is,\n",
    "```\n",
    "K = 1, lambda = C = 15,\n",
    "``` \n",
    "while the baseline model remains the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K = 1, c = 10\n"
     ]
    }
   ],
   "source": [
    "# Choose k and lambda that appear most frequently in the crossvalidation table\n",
    "k_optimals = list(cross_val_table['KNN_k_i'])\n",
    "c_optimals = list(cross_val_table['reg_lambda_i'])\n",
    "\n",
    "K = max(set(k_optimals), key = k_optimals.count)\n",
    "c = max(set(c_optimals), key = c_optimals.count)\n",
    "print(f\"K = {K}, c = {c}\")\n",
    "\n",
    "# Create models\n",
    "dummy = DummyClassifier(strategy='most_frequent')\n",
    "knn = KNeighborsClassifier(n_neighbors=K)\n",
    "reg = LogisticRegression(C=c, multi_class='multinomial')\n",
    "\n",
    "\n",
    "# Split data into random training and testing samples, we use normal holdout and train all models on the same splits\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "\n",
    "# Train classifiers predict\n",
    "y_pred_dummy = dummy.fit(x_train, y_train).predict(x_test)\n",
    "y_pred_knn = knn.fit(x_train, y_train).predict(x_test)\n",
    "y_pred_reg = reg.fit(x_train, y_train).predict(x_test)\n",
    "\n",
    "# Create list of 2-tuples with pairs to compare predictions from\n",
    "# That is: (kNN, baseline), (kNN, regression), (regression, baseline)\n",
    "prediction_pairs = [(y_pred_knn, y_pred_dummy), (y_pred_knn, y_pred_reg), (y_pred_reg, y_pred_knn), (y_pred_reg, y_pred_dummy)] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Some theory:**\n",
    "(Algorithm taken from: Tue Herlau, Mikkel N. Schmidt and Morten Mørup: \"Introduction to Machine Learning and Data Mining\", Lecture notes, Spring 2022, version 1.0, p.204 - 207.)\n",
    "\n",
    "##### **1. Contingency table**\n",
    "A contingency table is a tabulation or count of two categorical variables. In the case of the McNemar’s test, we are interested in binary variables correct/incorrect or yes/no for a control and a treatment or two cases. This is called a 2×2 contingency table. Example for two classifiers/models, C_1 and C_2, is given below:\n",
    "\n",
    "|               | C_2 correct | C_2 incorrect |\n",
    "|---------------|-------------|---------------|\n",
    "| **C_1 correct**   |      $n_{11}$      |       $n_{12}$       |\n",
    "| **C_1 incorrect** |      $n_{21}$      |       $n_{22}$       |\n",
    "\n",
    "##### **2. Hypothesis testing and the p-value**\n",
    "Whats interesting is when our to classifiers *differs* in their predictions. Therefore, we say that C_1 is better than C_2 when $n_{12} > n_{21}$ . Note that if $n_{12} > n_{21}$, then \n",
    "\n",
    "$$\\hat{r} = \\frac{n_{12}}{(n_{12} + n_{21})} > \\frac{1}{2}.$$\n",
    "\n",
    "We want to test the null-hypotheses that is $p_{12} = p_{21}$, i.e., \n",
    "\n",
    "$$r = \\frac{p_{12}}{p_{12} + p_{21}} = \\frac{1}{2}.$$\n",
    "\n",
    "We let our hypothesis' be:<br/>\n",
    "$H_{0}$: C_1 has the same performance as C_2.<br/>\n",
    "$H_{1}$: C_1 and C_2 have different performance.\n",
    "\n",
    "Recal that the p-value is defined as the probability of observing a value of $n_{12}$ and $n_{21}$ more extreme, i.e. unlikely, than the one actually observed assuming H0 is true. That is, $r = \\frac{1}{2}$. If we consider $m = min\\{n_{12},n_{21}\\}$ as the more extreme value, then we have that the p-value is\n",
    "\n",
    "$$p = 2cdf_{binom}(m|\\theta = \\frac{1}{2}, N = n_{12} + n_{21}).$$\n",
    "\n",
    "The lower the p-value, the more better is C_1 than C_2.\n",
    "\n",
    "##### **3. Confidence interval for the McNemar's test**\n",
    "We use p-values to see if C_1 is better than C_2, but we need a *confidence interval* to determine an interval that the performance difference most likely lies in. We therefore let $\\theta_i$ with $i = 1,2$ denote the true chance that the classifier C_i is correct. We want to arrive at an interval for the difference in performance for C_1 and C_2. Namely:\n",
    "\n",
    "$$ \\theta = \\theta_1 - \\theta_2$$\n",
    "\n",
    "Where $\\theta > 0$ indicates that C_1 is better (more accurate) than C_2, and vice versa. We thus let the *estimated* difference in accuracy between C_1 and C_2 be denoted $\\hat{\\theta}$, which is given by:\n",
    "\n",
    "$$ \\hat{\\theta} = \\frac{n_{12} - n_{21}}{n} $$\n",
    "\n",
    "where $n$ is the length of the test dataset. \n",
    "\n",
    "An *approximation* of the $(1 - \\alpha)$-confidence nterval, $[\\theta_L, \\theta_U]$, is then given by\n",
    "\n",
    "$$\\theta_L = 2cdf_B^{-1}(\\frac{\\alpha}{2}|\\alpha = f, \\beta = g) - 1$$\n",
    "$$\\theta_U = 2cdf_B^{-1}(1 - \\frac{\\alpha}{2}|\\alpha = f, \\beta = g) - 1$$\n",
    "\n",
    "where $cdf_B^{-1}()$ is the inverse cummulative Beta-distribution function, and $f()$ and $g()$ is given by\n",
    "\n",
    "$$f = \\frac{E_{\\theta} + 1}{2}(Q-1), \\quad g = \\frac{1- E_{\\theta}}{2}(Q-1)$$\n",
    "$$E_{\\theta} = \\frac{n_{12} - n_{21}}{n}, \\quad Q = \\frac{n^2(n+1)(E_{\\theta} + 1)(1 - E_{\\theta})}{n(n_{12}+n_{21})-(n_{12}-n_{21})^2}$$\n",
    "\n",
    "Note that the approximation of the CI only is useful when $n_{12} + n_{21} > 5$. This is true in our case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import binom, beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create contingency table\n",
    "def contingency_table(y_pred_1, y_pred_2, y_true):\n",
    "    \"\"\"Creates a contingency table for two classifiers that has produced \n",
    "    two predictions for the binary target y_true. Assumes all input same shape\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_pred_1 : One dimentional np.array\n",
    "        Prediction vector for model 1\n",
    "    \n",
    "    y_pred_2 : One dimentional np.array\n",
    "        Prediction vector for model 2\n",
    "    \n",
    "    y_true : One dimentional vector\n",
    "        True labels for the target variable\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    cont_table : 2x2 matrix\n",
    "        The contingency table for the two models\n",
    "    \"\"\"\n",
    "    correct_preds_1 = [1 if (y_pred_1[i] == y_true[i]) else 0 for i in range(len(y_pred_1))]\n",
    "    correct_preds_2 = [1 if (y_pred_2[i] == y_true[i]) else 0 for i in range(len(y_pred_2))]\n",
    "    \n",
    "    n11, n12, n21, n22 = 0, 0, 0, 0\n",
    "    \n",
    "    for j in range(len(y_true)):\n",
    "        n11 += correct_preds_1[j]*correct_preds_2[j]\n",
    "        n12 += correct_preds_1[j]*(1-correct_preds_2[j])\n",
    "        n21 += (1-correct_preds_1[j])*correct_preds_2[j]\n",
    "        n22 += (1-correct_preds_1[j])*(1-correct_preds_2[j])\n",
    "    \n",
    "    return np.array([[n11, n12],[n21, n22]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_value(n12, n21, theta):\n",
    "    \"\"\"Creates p-value based on the binomial distribution, given some contingency table (w/ n12, n21) and a theta\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    n12, n21: int\n",
    "        Number of times classifier i was correct and classifier j incorrect\n",
    "    theta: real\n",
    "        Theta, probability of a single success\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    p: real\n",
    "        p-value\n",
    "    \"\"\"\n",
    "    m = min(n12, n21)\n",
    "    N = n12 + n21\n",
    "    p = 2*binom.cdf(m, N, theta)\n",
    "    return p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confidence_interval(alpha, n12, n21, n):\n",
    "    \"\"\"Creates an approximation (via the beta distribution) of the (1-alpha)-confidence interval for the estimated theta\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    alpha: real\n",
    "        Threshold for (1-alpha) confidence interval\n",
    "    n12, n21: int\n",
    "        Number of times classifier i was correct and classifier j incorrect\n",
    "    n: int\n",
    "        Number of predictions\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    theta_L: real\n",
    "        Lower bound for the confidence interval approximation\n",
    "    theta_U: real\n",
    "        Upper bound for the confidence interval approximation\n",
    "    \"\"\"\n",
    "    E_theta = (n12 - n21)/n\n",
    "    Q = ((n**2)*(n+1)*(E_theta + 1)*(1 - E_theta))/(n*(n12 + n21) - (n12 - n21)**2)\n",
    "    f = ((E_theta + 1)/2)*(Q-1)\n",
    "    g = ((1- E_theta)/2)*(Q-1)\n",
    "    theta_L = 2*beta.ppf(alpha/2, f, g) - 1 #.ppf() calculates the inverse cummulative distribution function\n",
    "    theta_U = 2*beta.ppf(1 - (alpha/2), f, g) - 1\n",
    "    return theta_L, theta_U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run algorithm: \"The McNemar test for comparing classifiers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(y_pred_knn)\n",
    "alpha = 0.05 # Standard. Yields a 95% confidence interval.\n",
    "\n",
    "# Names of pairs of models to compare\n",
    "pairs_names = [\"(kNN, baseline)\", \"(kNN, regression)\", \"(regression, kNN)\", \"(regression, baseline)\"]\n",
    "\n",
    "# Define resulting dataframe\n",
    "res = pd.DataFrame(columns=[\"pair\", \"estimated difference\", \"CI lower\", \"CI upper\", \"p-value\"])\n",
    "res[\"pair\"] = pairs_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pairwise contingency tables:\n",
      "-----------------------------\n",
      "Pair = (kNN, baseline):\n",
      "[[123 118]\n",
      " [  4   9]]\n",
      "Pair = (kNN, regression):\n",
      "[[221  20]\n",
      " [  6   7]]\n",
      "Pair = (regression, kNN):\n",
      "[[221   6]\n",
      " [ 20   7]]\n",
      "Pair = (regression, baseline):\n",
      "[[121 106]\n",
      " [  6  21]]\n"
     ]
    }
   ],
   "source": [
    "# Create pairwise contingency tables:\n",
    "contingency_tables = [] \n",
    "y_true = np.array(y_test)\n",
    "for (pred1, pred2) in prediction_pairs:\n",
    "    table = contingency_table(pred1, pred2, y_true)\n",
    "    contingency_tables.append(table)\n",
    "\n",
    "print(\"Pairwise contingency tables:\")\n",
    "print(\"-----------------------------\")\n",
    "for i in range(len(contingency_tables)):\n",
    "    print(f\"Pair = {pairs_names[i]}:\")\n",
    "    print(contingency_tables[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpack all n12 and n21 as 2-tuples (n12, n21) for all contingency tables\n",
    "ns = []\n",
    "for t in contingency_tables:\n",
    "    ns.append((t[0][1], t[1][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimated difference column\n",
    "est_diff = []\n",
    "for (n12, n21) in ns:\n",
    "    est_ = (n12 - n21) / n\n",
    "    est_diff.append(est_)\n",
    "\n",
    "res[\"estimated difference\"] = est_diff\n",
    "\n",
    "# CI lower and upper columns\n",
    "CI_lowers = []\n",
    "CI_uppers = []\n",
    "for (n12, n21) in ns:\n",
    "    CI_lower, CI_upper = confidence_interval(alpha, n12, n21, n)\n",
    "    CI_lowers.append(CI_lower)\n",
    "    CI_uppers.append(CI_upper)\n",
    "\n",
    "res[\"CI lower\"] = CI_lowers\n",
    "res[\"CI upper\"] = CI_uppers\n",
    "\n",
    "# p-value column\n",
    "p_vals = []\n",
    "for (n12, n21) in ns:\n",
    "    p = p_value(n12, n21, theta=0.5)\n",
    "    p_vals.append(round(p, 6))\n",
    "\n",
    "res[\"p-value\"] = p_vals\n",
    "\n",
    "# Add column for whether we can reject the null hypothesis or not\n",
    "rej_h0 = [bool(p_vals[i] < alpha) for i in range(len(p_vals))]\n",
    "res[\"reject H0\"] = rej_h0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pair</th>\n",
       "      <th>estimated difference</th>\n",
       "      <th>CI lower</th>\n",
       "      <th>CI upper</th>\n",
       "      <th>p-value</th>\n",
       "      <th>reject H0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(kNN, baseline)</td>\n",
       "      <td>0.448819</td>\n",
       "      <td>0.382863</td>\n",
       "      <td>0.512443</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(kNN, regression)</td>\n",
       "      <td>0.055118</td>\n",
       "      <td>0.016399</td>\n",
       "      <td>0.093756</td>\n",
       "      <td>0.009355</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(regression, kNN)</td>\n",
       "      <td>-0.055118</td>\n",
       "      <td>-0.093756</td>\n",
       "      <td>-0.016399</td>\n",
       "      <td>0.009355</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(regression, baseline)</td>\n",
       "      <td>0.393701</td>\n",
       "      <td>0.327101</td>\n",
       "      <td>0.458318</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     pair  estimated difference  CI lower  CI upper   p-value  \\\n",
       "0         (kNN, baseline)              0.448819  0.382863  0.512443  0.000000   \n",
       "1       (kNN, regression)              0.055118  0.016399  0.093756  0.009355   \n",
       "2       (regression, kNN)             -0.055118 -0.093756 -0.016399  0.009355   \n",
       "3  (regression, baseline)              0.393701  0.327101  0.458318  0.000000   \n",
       "\n",
       "   reject H0  \n",
       "0       True  \n",
       "1       True  \n",
       "2       True  \n",
       "3       True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create nex train test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and predict with suitable lambda (c)\n",
    "c = 15 #double chech the ranges for lambda, might perform better if smaller interval with closer together values\n",
    "reg = LogisticRegression(C=c, multi_class='multinomial')\n",
    "y_pred = reg.fit(x_train, y_train).predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.889763779527559\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x1d0184b9850>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluation\n",
    "acc = reg.score(x_test, y_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "#maybe have some sort of f1 score for multiclass?\n",
    "\n",
    "print(f\"Accuracy score: {acc}\")\n",
    "cm_disp = ConfusionMatrixDisplay(cm)\n",
    "cm_disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3aa2052e9a08f0910284e2d7e8b5954f4401c8e958dc175eb29fb1cae388f6ea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
